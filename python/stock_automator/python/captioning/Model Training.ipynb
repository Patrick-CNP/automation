{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# We'll generate plots of attention in order to see which parts of an image\n",
    "# our model focuses on during captioning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn includes many helpful utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
    "                                          cache_subdir=os.path.abspath('.'),\n",
    "                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
    "                                          extract = True)\n",
    "annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
    "\n",
    "name_of_zip = 'train2014.zip'\n",
    "if not os.path.exists(os.path.abspath('.') + '/' + name_of_zip):\n",
    "  image_zip = tf.keras.utils.get_file(name_of_zip,\n",
    "                                      cache_subdir=os.path.abspath('.'),\n",
    "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                      extract = True)\n",
    "  PATH = os.path.dirname(image_zip)+'/train2014/'\n",
    "else:\n",
    "  PATH = os.path.abspath('.')+'/train2014/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the json file\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# storing the captions and the image name in vectors\n",
    "all_captions = []\n",
    "all_img_name_vector = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
    "    image_id = annot['image_id']\n",
    "    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
    "\n",
    "    all_img_name_vector.append(full_coco_image_path)\n",
    "    all_captions.append(caption)\n",
    "\n",
    "# shuffling the captions and image_names together\n",
    "# setting a random state\n",
    "train_captions, img_name_vector = shuffle(all_captions,\n",
    "                                          all_img_name_vector,\n",
    "                                          random_state=1)\n",
    "\n",
    "# selecting the first 30000 captions from the shuffled set\n",
    "num_examples =  50000\n",
    "train_captions = train_captions[:num_examples]\n",
    "img_name_vector = img_name_vector[:num_examples]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 414113)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_captions), len(all_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# getting the unique images\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# feel free to change the batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# c = 0;\n",
    "# for img, path in tqdm(image_dataset):\n",
    "#     already_processed = 0\n",
    "#     for p in path:\n",
    "#         path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "#         path_of_feature += \".npy\"\n",
    "#         exists = os.path.isfile(path_of_feature)\n",
    "#         if exists: \n",
    "#             already_processed += 1\n",
    "    \n",
    "#     if(already_processed == len(path)):\n",
    "#         continue\n",
    "        \n",
    "    \n",
    "#     batch_features = image_features_extract_model(img)\n",
    "#     batch_features = tf.reshape(batch_features,\n",
    "#                               (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    \n",
    "    \n",
    "#     for bf, p in zip(batch_features, path):\n",
    "#         path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "#         np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not_created = 0;\n",
    "# c = 0\n",
    "# for img, path in image_dataset:\n",
    "#     for p in path:\n",
    "#         path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "#         path_of_feature += \".npy\"\n",
    "#         exists = os.path.isfile(path_of_feature)\n",
    "#         if(not exists):\n",
    "#             not_created += 1\n",
    "#     c += 1\n",
    "#     if(c == 1000):\n",
    "#         break\n",
    "# print(not_created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The steps above is a general process of dealing with text processing\n",
    "\n",
    "# choosing the top 5000 words from the vocabulary\n",
    "top_k = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
    "                                                  oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(train_captions)\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the tokenized vectors\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding each vector to the max_length of the captions\n",
    "# if the max_length parameter is not provided, pad_sequences calculates that automatically\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the max_length\n",
    "# used to store the attention weights\n",
    "max_length = calc_max_length(train_seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation sets using 80-20 split\n",
    "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
    "                                                                    cap_vector,\n",
    "                                                                    test_size=0.2,\n",
    "                                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 40000, 10000, 10000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to change these parameters according to your system's configuration\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "BUFFER_SIZE = 16\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "# shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# these two variables represent that\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the numpy files\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0427 17:02:06.219327 140095657047360 deprecation.py:323] From /usr/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py:476: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# using map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# shuffling and batching\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "        # hidden shape == (batch_size, hidden_size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "        # score shape == (batch_size, 64, hidden_size)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "\n",
    "        # attention_weights shape == (batch_size, 64, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since we have already extracted the features and dumped it using pickle\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention= BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding this in a separate cell because if you run the training cell\n",
    "# many times, the loss_plot array will be reset\n",
    "loss_plot = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "\n",
    "    # initializing the hidden state for each batch\n",
    "    # because the captions are not related from image to image\n",
    "    hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            # passing the features through the decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.1666\n",
      "Epoch 1 Batch 100 Loss 1.1270\n",
      "Epoch 1 Batch 200 Loss 1.5403\n",
      "Epoch 1 Batch 300 Loss 1.2393\n",
      "Epoch 1 Batch 400 Loss 1.1326\n",
      "Epoch 1 Batch 500 Loss 0.9751\n",
      "Epoch 1 Batch 600 Loss 1.4197\n",
      "Epoch 1 Batch 700 Loss 1.0310\n",
      "Epoch 1 Batch 800 Loss 0.8124\n",
      "Epoch 1 Batch 900 Loss 0.8237\n",
      "Epoch 1 Batch 1000 Loss 1.4224\n",
      "Epoch 1 Batch 1100 Loss 1.3363\n",
      "Epoch 1 Batch 1200 Loss 0.8031\n",
      "Epoch 1 Batch 1300 Loss 0.8819\n",
      "Epoch 1 Batch 1400 Loss 0.7998\n",
      "Epoch 1 Batch 1500 Loss 1.1978\n",
      "Epoch 1 Batch 1600 Loss 0.9723\n",
      "Epoch 1 Batch 1700 Loss 1.0141\n",
      "Epoch 1 Batch 1800 Loss 1.1454\n",
      "Epoch 1 Batch 1900 Loss 0.6374\n",
      "Epoch 1 Batch 2000 Loss 1.0433\n",
      "Epoch 1 Batch 2100 Loss 0.8434\n",
      "Epoch 1 Batch 2200 Loss 0.7853\n",
      "Epoch 1 Batch 2300 Loss 0.6952\n",
      "Epoch 1 Batch 2400 Loss 0.7157\n",
      "Epoch 1 Batch 2500 Loss 0.8175\n",
      "Epoch 1 Batch 2600 Loss 1.7585\n",
      "Epoch 1 Batch 2700 Loss 1.1066\n",
      "Epoch 1 Batch 2800 Loss 1.3205\n",
      "Epoch 1 Batch 2900 Loss 1.0420\n",
      "Epoch 1 Batch 3000 Loss 1.0287\n",
      "Epoch 1 Batch 3100 Loss 0.9321\n",
      "Epoch 1 Batch 3200 Loss 1.1373\n",
      "Epoch 1 Batch 3300 Loss 1.2915\n",
      "Epoch 1 Batch 3400 Loss 0.8010\n",
      "Epoch 1 Batch 3500 Loss 0.9288\n",
      "Epoch 1 Batch 3600 Loss 0.9370\n",
      "Epoch 1 Batch 3700 Loss 1.0159\n",
      "Epoch 1 Batch 3800 Loss 1.0202\n",
      "Epoch 1 Batch 3900 Loss 0.8298\n",
      "Epoch 1 Batch 4000 Loss 0.9587\n",
      "Epoch 1 Batch 4100 Loss 1.0912\n",
      "Epoch 1 Batch 4200 Loss 1.0161\n",
      "Epoch 1 Batch 4300 Loss 0.9006\n",
      "Epoch 1 Batch 4400 Loss 1.0829\n",
      "Epoch 1 Batch 4500 Loss 1.1556\n",
      "Epoch 1 Batch 4600 Loss 0.6749\n",
      "Epoch 1 Batch 4700 Loss 1.0066\n",
      "Epoch 1 Batch 4800 Loss 1.3683\n",
      "Epoch 1 Batch 4900 Loss 0.9970\n",
      "Epoch 1 Batch 5000 Loss 1.2655\n",
      "Epoch 1 Batch 5100 Loss 0.9133\n",
      "Epoch 1 Batch 5200 Loss 0.7471\n",
      "Epoch 1 Batch 5300 Loss 1.0420\n",
      "Epoch 1 Batch 5400 Loss 1.1385\n",
      "Epoch 1 Batch 5500 Loss 1.0891\n",
      "Epoch 1 Batch 5600 Loss 0.4142\n",
      "Epoch 1 Batch 5700 Loss 0.6313\n",
      "Epoch 1 Batch 5800 Loss 0.8677\n",
      "Epoch 1 Batch 5900 Loss 0.8688\n",
      "Epoch 1 Batch 6000 Loss 1.0441\n",
      "Epoch 1 Batch 6100 Loss 0.7315\n",
      "Epoch 1 Batch 6200 Loss 0.8562\n",
      "Epoch 1 Batch 6300 Loss 1.0200\n",
      "Epoch 1 Batch 6400 Loss 1.6509\n",
      "Epoch 1 Batch 6500 Loss 0.7015\n",
      "Epoch 1 Batch 6600 Loss 1.3992\n",
      "Epoch 1 Batch 6700 Loss 0.7979\n",
      "Epoch 1 Batch 6800 Loss 0.9511\n",
      "Epoch 1 Batch 6900 Loss 0.9979\n",
      "Epoch 1 Batch 7000 Loss 0.8814\n",
      "Epoch 1 Batch 7100 Loss 0.6957\n",
      "Epoch 1 Batch 7200 Loss 0.6380\n",
      "Epoch 1 Batch 7300 Loss 1.2063\n",
      "Epoch 1 Batch 7400 Loss 1.2766\n",
      "Epoch 1 Batch 7500 Loss 0.8077\n",
      "Epoch 1 Batch 7600 Loss 1.4525\n",
      "Epoch 1 Batch 7700 Loss 0.7813\n",
      "Epoch 1 Batch 7800 Loss 0.5171\n",
      "Epoch 1 Batch 7900 Loss 0.9452\n",
      "Epoch 1 Batch 8000 Loss 0.7448\n",
      "Epoch 1 Batch 8100 Loss 0.7999\n",
      "Epoch 1 Batch 8200 Loss 0.9884\n",
      "Epoch 1 Batch 8300 Loss 0.3776\n",
      "Epoch 1 Batch 8400 Loss 1.2511\n",
      "Epoch 1 Batch 8500 Loss 0.8144\n",
      "Epoch 1 Batch 8600 Loss 0.8996\n",
      "Epoch 1 Batch 8700 Loss 0.8581\n",
      "Epoch 1 Batch 8800 Loss 0.8976\n",
      "Epoch 1 Batch 8900 Loss 0.4798\n",
      "Epoch 1 Batch 9000 Loss 0.9182\n",
      "Epoch 1 Batch 9100 Loss 0.7447\n",
      "Epoch 1 Batch 9200 Loss 0.7147\n",
      "Epoch 1 Batch 9300 Loss 1.1278\n",
      "Epoch 1 Batch 9400 Loss 0.9331\n",
      "Epoch 1 Batch 9500 Loss 0.9449\n",
      "Epoch 1 Batch 9600 Loss 0.5836\n",
      "Epoch 1 Batch 9700 Loss 1.7655\n",
      "Epoch 1 Batch 9800 Loss 1.3448\n",
      "Epoch 1 Batch 9900 Loss 0.9201\n",
      "Epoch 1 Batch 10000 Loss 0.7228\n",
      "Epoch 1 Batch 10100 Loss 0.4965\n",
      "Epoch 1 Batch 10200 Loss 0.8763\n",
      "Epoch 1 Batch 10300 Loss 1.2306\n",
      "Epoch 1 Batch 10400 Loss 0.9286\n",
      "Epoch 1 Batch 10500 Loss 1.0457\n",
      "Epoch 1 Batch 10600 Loss 0.6576\n",
      "Epoch 1 Batch 10700 Loss 0.7095\n",
      "Epoch 1 Batch 10800 Loss 0.9816\n",
      "Epoch 1 Batch 10900 Loss 0.7174\n",
      "Epoch 1 Batch 11000 Loss 0.9046\n",
      "Epoch 1 Batch 11100 Loss 0.5768\n",
      "Epoch 1 Batch 11200 Loss 0.4829\n",
      "Epoch 1 Batch 11300 Loss 0.4967\n",
      "Epoch 1 Batch 11400 Loss 1.2632\n",
      "Epoch 1 Batch 11500 Loss 0.4836\n",
      "Epoch 1 Batch 11600 Loss 0.6016\n",
      "Epoch 1 Batch 11700 Loss 1.0078\n",
      "Epoch 1 Batch 11800 Loss 0.7241\n",
      "Epoch 1 Batch 11900 Loss 0.9275\n",
      "Epoch 1 Batch 12000 Loss 0.8013\n",
      "Epoch 1 Batch 12100 Loss 0.7133\n",
      "Epoch 1 Batch 12200 Loss 0.8289\n",
      "Epoch 1 Batch 12300 Loss 0.7944\n",
      "Epoch 1 Batch 12400 Loss 0.5927\n",
      "Epoch 1 Batch 12500 Loss 0.8117\n",
      "Epoch 1 Batch 12600 Loss 0.4347\n",
      "Epoch 1 Batch 12700 Loss 0.5937\n",
      "Epoch 1 Batch 12800 Loss 0.4234\n",
      "Epoch 1 Batch 12900 Loss 1.0249\n",
      "Epoch 1 Batch 13000 Loss 0.7242\n",
      "Epoch 1 Batch 13100 Loss 0.4627\n",
      "Epoch 1 Batch 13200 Loss 1.4759\n",
      "Epoch 1 Batch 13300 Loss 0.6588\n",
      "Epoch 1 Batch 13400 Loss 0.7102\n",
      "Epoch 1 Batch 13500 Loss 0.8717\n",
      "Epoch 1 Batch 13600 Loss 0.7418\n",
      "Epoch 1 Batch 13700 Loss 0.8278\n",
      "Epoch 1 Batch 13800 Loss 0.6545\n",
      "Epoch 1 Batch 13900 Loss 0.7863\n",
      "Epoch 1 Batch 14000 Loss 0.8967\n",
      "Epoch 1 Batch 14100 Loss 0.9415\n",
      "Epoch 1 Batch 14200 Loss 0.8421\n",
      "Epoch 1 Batch 14300 Loss 0.5974\n",
      "Epoch 1 Batch 14400 Loss 0.6769\n",
      "Epoch 1 Batch 14500 Loss 0.8304\n",
      "Epoch 1 Batch 14600 Loss 0.6440\n",
      "Epoch 1 Batch 14700 Loss 1.0873\n",
      "Epoch 1 Batch 14800 Loss 0.7530\n",
      "Epoch 1 Batch 14900 Loss 0.5855\n",
      "Epoch 1 Batch 15000 Loss 0.6447\n",
      "Epoch 1 Batch 15100 Loss 1.0067\n",
      "Epoch 1 Batch 15200 Loss 0.8056\n",
      "Epoch 1 Batch 15300 Loss 0.7800\n",
      "Epoch 1 Batch 15400 Loss 0.7856\n",
      "Epoch 1 Batch 15500 Loss 1.7486\n",
      "Epoch 1 Batch 15600 Loss 0.6655\n",
      "Epoch 1 Batch 15700 Loss 0.6246\n",
      "Epoch 1 Batch 15800 Loss 0.7166\n",
      "Epoch 1 Batch 15900 Loss 0.5922\n",
      "Epoch 1 Batch 16000 Loss 0.6285\n",
      "Epoch 1 Batch 16100 Loss 0.8128\n",
      "Epoch 1 Batch 16200 Loss 0.8838\n",
      "Epoch 1 Batch 16300 Loss 0.5281\n",
      "Epoch 1 Batch 16400 Loss 0.7956\n",
      "Epoch 1 Batch 16500 Loss 0.9890\n",
      "Epoch 1 Batch 16600 Loss 0.7054\n",
      "Epoch 1 Batch 16700 Loss 0.6489\n",
      "Epoch 1 Batch 16800 Loss 0.5798\n",
      "Epoch 1 Batch 16900 Loss 1.0066\n",
      "Epoch 1 Batch 17000 Loss 0.9753\n",
      "Epoch 1 Batch 17100 Loss 1.2745\n",
      "Epoch 1 Batch 17200 Loss 0.8527\n",
      "Epoch 1 Batch 17300 Loss 0.6350\n",
      "Epoch 1 Batch 17400 Loss 0.5801\n",
      "Epoch 1 Batch 17500 Loss 1.0160\n",
      "Epoch 1 Batch 17600 Loss 0.9647\n",
      "Epoch 1 Batch 17700 Loss 0.7074\n",
      "Epoch 1 Batch 17800 Loss 0.7941\n",
      "Epoch 1 Batch 17900 Loss 0.6087\n",
      "Epoch 1 Batch 18000 Loss 0.7144\n",
      "Epoch 1 Batch 18100 Loss 0.6304\n",
      "Epoch 1 Batch 18200 Loss 0.5416\n",
      "Epoch 1 Batch 18300 Loss 1.0280\n",
      "Epoch 1 Batch 18400 Loss 0.4400\n",
      "Epoch 1 Batch 18500 Loss 0.8795\n",
      "Epoch 1 Batch 18600 Loss 0.9193\n",
      "Epoch 1 Batch 18700 Loss 0.9057\n",
      "Epoch 1 Batch 18800 Loss 0.8603\n",
      "Epoch 1 Batch 18900 Loss 0.5927\n",
      "Epoch 1 Batch 19000 Loss 1.0214\n",
      "Epoch 1 Batch 19100 Loss 0.8536\n",
      "Epoch 1 Batch 19200 Loss 0.6018\n",
      "Epoch 1 Batch 19300 Loss 0.4720\n",
      "Epoch 1 Batch 19400 Loss 0.6699\n",
      "Epoch 1 Batch 19500 Loss 0.8015\n",
      "Epoch 1 Batch 19600 Loss 0.7623\n",
      "Epoch 1 Batch 19700 Loss 1.2817\n",
      "Epoch 1 Batch 19800 Loss 0.5735\n",
      "Epoch 1 Batch 19900 Loss 0.8640\n",
      "Epoch 1 Loss 0.864087\n",
      "Time taken for 1 epoch 18281.819349765778 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.7495\n",
      "Epoch 2 Batch 100 Loss 0.6007\n",
      "Epoch 2 Batch 200 Loss 1.2159\n",
      "Epoch 2 Batch 300 Loss 0.7759\n",
      "Epoch 2 Batch 400 Loss 0.7240\n",
      "Epoch 2 Batch 500 Loss 0.6603\n",
      "Epoch 2 Batch 600 Loss 0.9689\n",
      "Epoch 2 Batch 700 Loss 0.6542\n",
      "Epoch 2 Batch 800 Loss 0.4537\n",
      "Epoch 2 Batch 900 Loss 0.6719\n",
      "Epoch 2 Batch 1000 Loss 0.9786\n",
      "Epoch 2 Batch 1100 Loss 0.9997\n",
      "Epoch 2 Batch 1200 Loss 0.6539\n",
      "Epoch 2 Batch 1300 Loss 0.6200\n",
      "Epoch 2 Batch 1400 Loss 0.6289\n",
      "Epoch 2 Batch 1500 Loss 0.9170\n",
      "Epoch 2 Batch 1600 Loss 0.7435\n",
      "Epoch 2 Batch 1700 Loss 0.9011\n",
      "Epoch 2 Batch 1800 Loss 0.8324\n",
      "Epoch 2 Batch 1900 Loss 0.5387\n",
      "Epoch 2 Batch 2000 Loss 0.9051\n",
      "Epoch 2 Batch 2100 Loss 0.5345\n",
      "Epoch 2 Batch 2200 Loss 0.5912\n",
      "Epoch 2 Batch 2300 Loss 0.6138\n",
      "Epoch 2 Batch 2400 Loss 0.5331\n",
      "Epoch 2 Batch 2500 Loss 0.6539\n",
      "Epoch 2 Batch 2600 Loss 1.6047\n",
      "Epoch 2 Batch 2700 Loss 1.0157\n",
      "Epoch 2 Batch 2800 Loss 1.2234\n",
      "Epoch 2 Batch 2900 Loss 0.8133\n",
      "Epoch 2 Batch 3000 Loss 0.7702\n",
      "Epoch 2 Batch 3100 Loss 0.9189\n",
      "Epoch 2 Batch 3200 Loss 0.9144\n",
      "Epoch 2 Batch 3300 Loss 1.0718\n",
      "Epoch 2 Batch 3400 Loss 0.5974\n",
      "Epoch 2 Batch 3500 Loss 0.7419\n",
      "Epoch 2 Batch 3600 Loss 0.8046\n",
      "Epoch 2 Batch 3700 Loss 0.8675\n",
      "Epoch 2 Batch 3800 Loss 0.8017\n",
      "Epoch 2 Batch 3900 Loss 0.7024\n",
      "Epoch 2 Batch 4000 Loss 0.8780\n",
      "Epoch 2 Batch 4100 Loss 0.9334\n",
      "Epoch 2 Batch 4200 Loss 0.7816\n",
      "Epoch 2 Batch 4300 Loss 0.8213\n",
      "Epoch 2 Batch 4400 Loss 1.0215\n",
      "Epoch 2 Batch 4500 Loss 0.9521\n",
      "Epoch 2 Batch 4600 Loss 0.6027\n",
      "Epoch 2 Batch 4700 Loss 0.9151\n",
      "Epoch 2 Batch 4800 Loss 1.2215\n",
      "Epoch 2 Batch 4900 Loss 0.8525\n",
      "Epoch 2 Batch 5000 Loss 1.0587\n",
      "Epoch 2 Batch 5100 Loss 0.7734\n",
      "Epoch 2 Batch 5200 Loss 0.6424\n",
      "Epoch 2 Batch 5300 Loss 0.8957\n",
      "Epoch 2 Batch 5400 Loss 1.0417\n",
      "Epoch 2 Batch 5500 Loss 0.8950\n",
      "Epoch 2 Batch 5600 Loss 0.4042\n",
      "Epoch 2 Batch 5700 Loss 0.6153\n",
      "Epoch 2 Batch 5800 Loss 0.7072\n",
      "Epoch 2 Batch 5900 Loss 0.7378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 6000 Loss 0.8622\n",
      "Epoch 2 Batch 6100 Loss 0.6225\n",
      "Epoch 2 Batch 6200 Loss 0.8093\n",
      "Epoch 2 Batch 6300 Loss 0.9138\n",
      "Epoch 2 Batch 6400 Loss 1.4503\n",
      "Epoch 2 Batch 6500 Loss 0.6352\n",
      "Epoch 2 Batch 6600 Loss 1.1758\n",
      "Epoch 2 Batch 6700 Loss 0.7185\n",
      "Epoch 2 Batch 6800 Loss 0.8856\n",
      "Epoch 2 Batch 6900 Loss 0.8488\n",
      "Epoch 2 Batch 7000 Loss 0.7880\n",
      "Epoch 2 Batch 7100 Loss 0.6274\n",
      "Epoch 2 Batch 7200 Loss 0.5667\n",
      "Epoch 2 Batch 7300 Loss 1.0257\n",
      "Epoch 2 Batch 7400 Loss 1.1138\n",
      "Epoch 2 Batch 7500 Loss 0.6762\n",
      "Epoch 2 Batch 7600 Loss 1.2898\n",
      "Epoch 2 Batch 7700 Loss 0.6917\n",
      "Epoch 2 Batch 7800 Loss 0.4487\n",
      "Epoch 2 Batch 7900 Loss 0.8752\n",
      "Epoch 2 Batch 8000 Loss 0.7044\n",
      "Epoch 2 Batch 8100 Loss 0.7052\n",
      "Epoch 2 Batch 8200 Loss 0.8209\n",
      "Epoch 2 Batch 8300 Loss 0.3507\n",
      "Epoch 2 Batch 8400 Loss 1.2121\n",
      "Epoch 2 Batch 8500 Loss 0.7227\n",
      "Epoch 2 Batch 8600 Loss 0.8287\n",
      "Epoch 2 Batch 8700 Loss 0.8623\n",
      "Epoch 2 Batch 8800 Loss 0.8608\n",
      "Epoch 2 Batch 8900 Loss 0.4408\n",
      "Epoch 2 Batch 9000 Loss 0.8780\n",
      "Epoch 2 Batch 9100 Loss 0.7333\n",
      "Epoch 2 Batch 9200 Loss 0.6235\n",
      "Epoch 2 Batch 9300 Loss 0.9699\n",
      "Epoch 2 Batch 9400 Loss 0.7877\n",
      "Epoch 2 Batch 9500 Loss 0.7780\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save_weights('./models/Encoder30kApril28_10epochs')\n",
    "decoder.save_weights('./models/Decoder30kApril28_10epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions on the validation set\n",
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "#plot_attention(image, result, attention_plot)\n",
    "# opening the image\n",
    "Image.open(img_name_val[rid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
    "image_extension = image_url[-4:]\n",
    "image_path = tf.keras.utils.get_file('image'+image_extension,\n",
    "                                     origin=image_url)\n",
    "\n",
    "result, attention_plot = evaluate(image_path)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image_path, result, attention_plot)\n",
    "# opening the image\n",
    "Image.open(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
